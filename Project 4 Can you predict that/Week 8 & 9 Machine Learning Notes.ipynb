{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 8 & 9 | Machine Learning\n",
    "\n",
    "- Let's try this out to see if this works well for taking notes! I bet it'll be better for showing code.\n",
    "- There is __classification__ and __prediction__\n",
    "- probably more than 4 features\n",
    "- too specific question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "Q. what does \"__one hot encode__\" mean?\n",
    "\n",
    "A. Taking categorical variables, pivoting to binary T/F for each of them \n",
    "\n",
    "Notes:\n",
    "- split datasets into features and targets\n",
    "\n",
    "- ```data.drop(columns = ['col1', 'col2'])``` to remove columns of data\n",
    "\n",
    "\n",
    "__Scikit-learn__ is the standard of ML\n",
    "  - We never load the whole package. Grab what we need. \n",
    "  - train_test_split:\n",
    "  - [ ] check out the reading assignments\n",
    "  - ```shuffle``` is a default argument. or ```random_state```\n",
    "  - Subtle difference: random state is like what set.seed does. With Shuffle, it's the order of the data which may be biast. \n",
    "  - methods quiz used 76\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "import numpy as np\n",
    "\n",
    "denver_dwelling = pd.read_csv(\"https://github.com/byuidatascience/data4dwellings/raw/master/data-raw/dwellings_ml/dwellings_ml.csv\")\n",
    "dwellings_ml = pd.read_csv(\"https://github.com/byuidatascience/data4dwellings/raw/master/data-raw/dwellings_ml/dwellings_ml.csv\")\n",
    "\n",
    "alt.data_transformers.enable('json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dwellings_ml.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denver_dwelling.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 8 Thursday Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import altair as alt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"X\" or \"independent\"\n",
    "features = dwellings_ml.drop(columns = ['before1980', 'yrbuilt','parcel'])\n",
    "\n",
    "# \"y\" or \"dependent\" or \"outcome\"\n",
    "targets = dwellings_ml.before1980\n",
    "\n",
    "# split the data!\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, targets, test_size = .34, random_state = 76)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "classifier = GaussianNB()\n",
    "\n",
    "# train the model\n",
    "classifier.fit(x_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "y_predicted = classifier.predict(x_test)\n",
    "\n",
    "# evaluate model (see how good the model is)\n",
    "metrics.accuracy_score(y_test, y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 9 Tuesday Code\n",
    "\n",
    "Steps from Gaussian classification model:\n",
    "- create the instance\n",
    "- train/fit\n",
    "\n",
    "\n",
    "Three things to try to up accuracy:\n",
    "- change variable used in features/x data\n",
    "  - dropping or adding variables\n",
    "- type of model\n",
    "  - decision tree\n",
    "  - gaussian\n",
    "- tune the parameters of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  kitchen sink approach\n",
    "features = dwellings_ml.drop(columns = ['before1980', 'yrbuilt', 'parcel'])\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, targets, test_size = 3, random_state = 24601)\n",
    "# x_train, x_test, y_train, y_test = train_test_split(features, targets, test_size = .34, random_state = 76)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "classifier_DT = DecisionTreeClassifier(max_depth = 5)\n",
    "\n",
    "# train the model\n",
    "classifier_DT.fit(x_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "y_predicted = classifier.predict(x_test)\n",
    "\n",
    "# evaluate model (see how good the model is)\n",
    "metrics.accuracy_score(y_test, y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the \"feature importance\". Aka, how important the model thinks the variables are.\n",
    "\n",
    "feature_df = pd.DataFrame({'features':features.columns, 'importance':classifier_DT.feature_importances_}).sort_values('importance', ascending = False)\n",
    "feature_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    alt.Chart(feature_df)\n",
    "    .mark_bar()\n",
    "    .encode(\n",
    "        y = alt.Y('features', sort = '-x'), \n",
    "        x = 'importance'\n",
    "        )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CoNfUsIoN Matrix\n",
    "# think False positives and false negatives. Type 1 and type two errors\n",
    "\n",
    "# metrics.\n",
    "\n",
    "# balanced accuracy! \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methods to measure success in models:\n",
    "- accuracy\n",
    "- balanced accuracy\n",
    "- recall\n",
    "- precision\n",
    "\n",
    "The _correct_ one depends on the scenario"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code from Tuesday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import altair as alt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a classification model\n",
    "classifier_DT = DecisionTreeClassifier(max_depth=33)\n",
    "\n",
    "# train the model\n",
    "classifier_DT.fit(x_train, y_train)\n",
    "\n",
    "# use your model to make predictions!\n",
    "y_predicted = classifier_DT.predict(x_test)\n",
    "\n",
    "# test how accurate those predictions are\n",
    "metrics.accuracy_score(y_test, y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grand Question 1 \n",
    "Create 2-3 charts that evaluate potential relationships between the house variables and the variable before1980 Explain what you learn from the charts that could help a machine learning algorithm.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dwellings_ml.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# year column and something else\n",
    "(\n",
    "    alt.Chart(dwellings_ml)\n",
    "    .mark_boxplot()\n",
    "    .encode(\n",
    "        y = alt.Y('yrbuilt:Q', \n",
    "        scale=alt.Scale(domain=[1800, 2020])\n",
    "        ), \n",
    "        x = alt.X('arcstyle_ONE AND HALF-STORY:O')\n",
    "        # , color = \"before1980:O\"\n",
    "        )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['parcel', 'abstrprd', 'livearea', 'finbsmnt', 'basement', 'yrbuilt',\n",
       "       'totunits', 'stories', 'nocars', 'numbdrm', 'numbaths', 'sprice',\n",
       "       'deduct', 'netprice', 'tasp', 'smonth', 'syear', 'condition_AVG',\n",
       "       'condition_Excel', 'condition_Fair', 'condition_Good',\n",
       "       'condition_VGood', 'quality_A', 'quality_B', 'quality_C', 'quality_D',\n",
       "       'quality_X', 'gartype_Att', 'gartype_Att/Det', 'gartype_CP',\n",
       "       'gartype_Det', 'gartype_None', 'gartype_att/CP', 'gartype_det/CP',\n",
       "       'arcstyle_BI-LEVEL', 'arcstyle_CONVERSIONS', 'arcstyle_END UNIT',\n",
       "       'arcstyle_MIDDLE UNIT', 'arcstyle_ONE AND HALF-STORY',\n",
       "       'arcstyle_ONE-STORY', 'arcstyle_SPLIT LEVEL', 'arcstyle_THREE-STORY',\n",
       "       'arcstyle_TRI-LEVEL', 'arcstyle_TRI-LEVEL WITH BASEMENT',\n",
       "       'arcstyle_TWO AND HALF-STORY', 'arcstyle_TWO-STORY', 'qualified_Q',\n",
       "       'qualified_U', 'status_I', 'status_V', 'before1980'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dwellings_ml.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-1f6a31d16af749a59bcf2cd3727d327f\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-1f6a31d16af749a59bcf2cd3727d327f\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-1f6a31d16af749a59bcf2cd3727d327f\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.8.1?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function loadScript(lib) {\n",
       "      return new Promise(function(resolve, reject) {\n",
       "        var s = document.createElement('script');\n",
       "        s.src = paths[lib];\n",
       "        s.async = true;\n",
       "        s.onload = () => resolve(paths[lib]);\n",
       "        s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "        document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "      });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else if (typeof vegaEmbed === \"function\") {\n",
       "      displayChart(vegaEmbed);\n",
       "    } else {\n",
       "      loadScript(\"vega\")\n",
       "        .then(() => loadScript(\"vega-lite\"))\n",
       "        .then(() => loadScript(\"vega-embed\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 400, \"continuousHeight\": 300}}, \"data\": {\"url\": \"altair-data-44362783edd7586208e38da934db71ed.json\", \"format\": {\"type\": \"json\"}}, \"mark\": \"circle\", \"encoding\": {\"color\": {\"type\": \"ordinal\", \"field\": \"before1980\"}, \"x\": {\"type\": \"quantitative\", \"field\": \"stories\"}, \"y\": {\"type\": \"quantitative\", \"field\": \"yrbuilt\", \"scale\": {\"domain\": [1800, 2020]}}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v4.8.1.json\"}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# year column and something else\n",
    "\n",
    "(\n",
    "    alt.Chart(dwellings_ml)\n",
    "    .mark_circle()\n",
    "    .encode(\n",
    "        y = alt.Y('yrbuilt:Q', \n",
    "        scale=alt.Scale(domain=[1800, 2020])\n",
    "        ), # zoom into the revelant years\n",
    "        x = alt.X('stories:Q'), color = \"before1980:O\"\n",
    "        )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grand Question 2\n",
    "Build a classification model labeling houses as being built “before 1980” or “during or after 1980”. Your goal is to reach 90% accuracy. Explain your final model choice (algorithm, tuning parameters, etc) and describe what other models you tried.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import altair as alt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import xgboost\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"X\" or \"independent\"\n",
    "features = dwellings_ml.drop(columns = ['before1980', 'yrbuilt','parcel'])\n",
    "\n",
    "# \"y\" or \"dependent\" or \"outcome\"\n",
    "targets = dwellings_ml.before1980\n",
    "# targets must already be boolean (classification type)\n",
    "\n",
    "\n",
    "# split the data!\n",
    "x_train, x_test, y_train, y_test = train_test_split(features, targets, test_size = .20, random_state = 76)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_DT = RandomForestClassifier(max_depth=30) #create a classification model\n",
    " # I fear that this is over fitting. How can I check for this?\n",
    "# classifier_DT = DecisionClassifier(max_depth=10)\n",
    "\n",
    "# train the model\n",
    "classifier_DT.fit(x_train, y_train)\n",
    "\n",
    "# use your model to make predictions!\n",
    "y_predicted = classifier_DT.predict(x_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# test how accurate those predictions are\n",
    "metrics.accuracy_score(y_test, y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I opted to do a random forest model. I have the training data set at 80% and the test data at 20%. The max depth of the model is at 10 which gives us 90% accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grand Question 3 \n",
    "Justify your classification model by discussing the most important features selected by your model. This discussion should include a chart and a description of the features.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_importances = pd.Series(\n",
    "    classifier_DT\n",
    "    .feature_importances_, index=x_test\n",
    "    .columns\n",
    ")\n",
    "\n",
    "significance_graph = (\n",
    "    feat_importances\n",
    "    .nlargest(10)\n",
    "    .plot(kind='barh')\n",
    ")\n",
    "significance_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df = pd.DataFrame({'features':features.columns, 'importance':classifier_DT.feature_importances_})\n",
    "\n",
    "feature_importance = feature_df.sort_values('importance', ascending = False).head()\n",
    "\n",
    "feature_importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shown above are the five most useful features of the model. The first two are related and that may cause issues but we know that we can choose one of them. The quality of the house (feature number three) is something humans would think are significant, and that is considered in the model. Having a garage is also considered significant. Living area, which is another no-brainer of a factor, is considered. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gq2chart = (\n",
    "alt.Chart(feature_importance)\n",
    ".mark_bar()\n",
    ".encode(\n",
    "    x = alt.X(\"features:O\"),\n",
    "    y = alt.Y(\"importance:Q\")\n",
    "))\n",
    "\n",
    "gq2chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grand Question 4\n",
    "Describe the quality of your classification model using 2-3 different evaluation metrics. You also need to explain how to interpret each of the evaluation metrics you use.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For evaluating model performance, there is accuracy, precision, and recall.  \n",
    "\n",
    "Recall often the most important\n",
    "\n",
    "Formulas: \n",
    "- Accuracy: (number of correct responses) / (total number of test cases)\n",
    "- Recall: (number of true positives) / (number of true positives + number of false negatives)\n",
    "- Precision: (number of true positives) / (number of true positives + number of false positives)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Fantastic Website to learn about machine learning evaluation metrics](https://ranvir.xyz/blog/how-to-evaluate-your-machine-learning-model-like-a-pro-metrics/#accuracy)\n",
    "\n",
    "**0. How Accurate is Your Model?**\n",
    "\n",
    "- What should be the answer to this question?\n",
    "\n",
    "\n",
    "**1. What is the process of evaluating your model?**\n",
    "\n",
    "- Starting with the original dataset, split into training data (80%) and test data (20%). \n",
    "- With the *training data*, put through a ML model of choice to train the model\n",
    "- After the model is trained, then the prediction result arrives. \n",
    "    - What is this one exactly? Just having the prediction?\n",
    "- With *test data*, compare it to the training data's prediction result, then evaluate (which is the point of this article).\n",
    "    - Compare the test data to a prediction result? How exactly? Isn't that a number compared to a dataset?\n",
    "\n",
    "[](!ML Diagram.png)\n",
    "\n",
    "\n",
    "- ?Evaluating a model is applying the linear regression model to unseen data?\n",
    "\n",
    "\n",
    "**2. Why do we need to evaluate our model?**\n",
    "- Evaluating a model is useful for those who don't speak tech\n",
    "- There is no perfect way of measuring the success of the model\n",
    "- Rule of thumb, play it safe when this decision counts\n",
    "- In the case of diagnosing cancer, we would rather tell the person that they have when they don't than tell the person they don't have cancer when they really do.\n",
    "\n",
    "\n",
    "**3. Supervised learning and classification problems.**\n",
    "- **Supervised learning** are problems where the outcomes of the model are already known\n",
    "    - A model is given a target (a Y variable) and it's task is to find patterns that help it detect what that target will be. \n",
    "    - It is then given that 20% of the data is hasn't seen to test it's self on how correct it's attempt was. \n",
    "- **Classification Problems** are a subset of supervised learning where the outcomes are divided into two or more parts; whether a person is having cancer or not\n",
    "\n",
    "- Both models can be evaluated on the following parameters _____??\n",
    "\n",
    "**3a. Test-Train-Split Using sklearn**\n",
    "- the general idea of this in code is to: \n",
    "    - **Grab the train_test_split function**: from sklearn.model_selection import train_test_split\n",
    "    - **read in data**: df = pd.read_csv('dataset')\n",
    "    - **Define features, otherwise known as X variables**: X = df[['Avg. Session Length', 'Time on App', 'Time on Website', 'Length of Membership']]\n",
    "    - **Define the target, otherwise known as the Y variable**: y = df['Yearly Amount Spent']\n",
    "    - **Split the data into four subsets**: X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "        - why four and not two?\n",
    "\n",
    "**4. Accuracy**\n",
    "- Accuracy is basically a score on a homework assignment or a test\n",
    "- it is the number of correct responses divided by the total number of test cases\n",
    "- insert MD fraction here\n",
    "- Pros:\n",
    "    - Easily understandable\n",
    "- Cons: \n",
    "    - this type of evaluation is not the best thing to use when the data available is unbalanced\n",
    "        - is this when the confusion matrix is unbalenced somehow?\n",
    "- To calculate accuracy: (I adapted the following code from other metrics of success code. It probably follow the same pattern but check it later to be safe)\n",
    "    - from sklearn.metrics import accuracy_score \n",
    "    - recall=accuracy_score(y_true,y_pred,average='binary')\n",
    "\n",
    "- This paragraph is currently unclear: \n",
    "    - \"We first calculate the predictions corresponding to the given X_test, finally, we compare these predictions with the y_test which are the real outcomes to the corresponding parameters.\"\n",
    "\n",
    "**5. Recall**\n",
    "- Recall is the ability of your model to find all the relevant cases in your model\n",
    "- I think this one is used when data is impalanced\n",
    "- the formula is the number of true positives divided by the number of true positives plus the number of false negatives\n",
    "- Recall often is used in tandum with the value of precision to ultimately measure the success. \n",
    "\n",
    "\n",
    "**6. Precision**\n",
    "\n",
    "\n",
    "\n",
    "**7. Accuracy and Recall using Venn Diagram**\n",
    "\n",
    "\n",
    "\n",
    "**8. Accuracy vs Recall using Example.**\n",
    "\n",
    "\n",
    "\n",
    "**9. F1-score**\n",
    "\n",
    "\n",
    "\n",
    "**10. Confusion matrix**\n",
    "\n",
    "\n",
    "\n",
    "**11. Evaluating for regression problems**\n",
    "\n",
    "\n",
    "\n",
    "**12. Mean Absolute Error**\n",
    "\n",
    "\n",
    "\n",
    "**13. Mean Square Error**\n",
    "\n",
    "\n",
    "\n",
    "**14. Root Mean Square Error**\n",
    "\n",
    "\n",
    "\n",
    "**15. What is considered as a good metric value for your model?**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix = confusion_matrix(y_test, y_predicted)\n",
    "\n",
    "confusion_matrix_df = pd.DataFrame(confusion_matrix, columns=[\"True Negatives\", \"False Positives\"]).reset_index()\n",
    "confusion_matrix_df\n",
    "\n",
    "# true_negative = confusion_matrix_df.query('index')\n",
    "# false_positive = \n",
    "# false_negative = \n",
    "# true_positive = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # this one might be easier to read\n",
    "# print(pd.crosstab(\n",
    "#             y_test.flatten(), \n",
    "#             y_predicted, \n",
    "#             rownames=['True'], \n",
    "#             colnames=['Predicted'], \n",
    "#             margins=True)\n",
    "# )\n",
    "\n",
    "# # visualize a confusion matrix\n",
    "# # requires 'matplotlib' to be installed\n",
    "# metrics.plot_confusion_matrix(classifier_DT, \n",
    "#                               x_test, \n",
    "#                               y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy Score \n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(y_test, classifier_DT.predict(x_test))\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall Score\n",
    "from sklearn.metrics import recall_score\n",
    "\n",
    "recall = recall_score(y_test, y_predicted, average='binary')\n",
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision Score\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "precision = precision_score(y_test, y_predicted, average='binary')\n",
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "classification_report = classification_report(y_test, y_predicted)\n",
    "print(classification_report)\n",
    "# classification_report.pd.dataframe()\n",
    "\n",
    "# classification_report_df = pd.DataFrame(classification_report, columns=[\"\", \"\", \"\", \"\", \"\", \"\"]).reset_index()\n",
    "# classification_report_df"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1829f6727cfd50cc2b28cfb4c3361c4127d6b937d4bd1de14de3ba12a0b61d85"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
